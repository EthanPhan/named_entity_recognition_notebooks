{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named Entity Recognition With Residual LSTM And BERT",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G3AJUAk1QwA",
        "colab_type": "text"
      },
      "source": [
        "In 2018 we saw the rise of pretraining and fine-tuning in natural language processing. Large neural networks have been trained on general tasks like language modelling and then fine-tuned for classification tasks. One of the latest milestones in this development is the release of BERT. BERT is a model that broke several records for how well models can handle language-based tasks. The model is based on a transformer architecture for “Attention is all you need”. They pre-trained it in a bidirectional way on several language modelling tasks. So probably the new slogan should read “Attention and pre-training is all you need”. If you want more details about the model and it’s pre-training, you find some resources [here](https://github.com/google-research/bert)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_jo8Pc1fEP",
        "colab_type": "text"
      },
      "source": [
        "### Data preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLKgQtVl07Ei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0b4eef4-de43-4d3e-ff0c-9acba69ba5f7"
      },
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "    \n",
        "# Download data\n",
        "url ='https://storage.googleapis.com/kaggle-datasets/1014/4361/entity-annotated-corpus.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1568009584&Signature=hSdo87OA6a0gwPDkkK1eHg1l3bfLg%2FO9CTZ%2Fma6B%2F%2BmcwwU7OQiEmjKpgJ8ROWbPXrwjhED3u3dkas63MRbL1Rin3XUeWKU3y6TqgK%2FmleA3SVf6jBqXTOfRjyDaPXPNYdJLYFCWIDbygZPxoNEmXel3ZV%2B3MQgDOKH%2FzAP1NLuU5y6VHaFePdsruHAb1KICRY6qvsl5gFTYyBkJw3xO0qoF8oNkG3C4uUDaTEaqVK7FOfAw7OkkpTXqc9GtjUdsI3Dr11QNYgTmIOdreqk0fgr89QaenXBTfZlS8hqMu46Ik1VrX0Y5zfOSH7Rd3T5ltDvNNANlh%2FA%2BpJr0y16cHA%3D%3D'\n",
        "\n",
        "urlretrieve(url, 'data/kaggle_ner.zip')\n",
        "\n",
        "with zipfile.ZipFile('data/kaggle_ner.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/')\n",
        "    \n",
        "import glob\n",
        "\n",
        "glob.glob('data/*')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/ner_dataset.csv', 'data/kaggle_ner.zip', 'data/ner.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyzvzjAJ1iLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd    \n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "\n",
        "class SentenceGetter(object):\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "        \n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences\n",
        "\n",
        "max_len = 50\n",
        "max_len_char = 10\n",
        "\n",
        "words = list(set(data[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "n_words = len(words)\n",
        "\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "n_tags = len(tags); n_tags\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcpwmd-s1nMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "7943f5c2-ec39-407e-eeee-1e594554f653"
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import tokenization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3KU6ZBUmaH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "5576d449-b787-441f-dd1b-a6c266651f3a"
      },
      "source": [
        "import tensorflow as tf\n",
        "#from tensorflow.python.framework.ops import disable_eager_execution\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#disable_eager_execution()\n",
        "\n",
        "\n",
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0906 07:21:35.690908 139686195558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV7A1G3Rxm59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "for sentence in sentences:\n",
        "  orig_to_tok_map = []\n",
        "  bert_tokens = []\n",
        "  ner_tag = []\n",
        "\n",
        "  bert_tokens.append(\"[CLS]\")\n",
        "  ner_tag.append('O')\n",
        "  \n",
        "  for orig_token in sentence:\n",
        "    orig_token, org_tag = orig_token[0], orig_token[2]\n",
        "    orig_to_tok_map.append(len(bert_tokens))\n",
        "    \n",
        "    tks = tokenizer.tokenize(orig_token)\n",
        "    bert_tokens.extend(tks)\n",
        "    ner_tag.extend([org_tag] * len(tks))\n",
        "  bert_tokens.append(\"[SEP]\")\n",
        "  ner_tag.append('O')\n",
        "  \n",
        "  X.append(bert_tokens)\n",
        "  y.append(ner_tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL7z7HqIyxnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 75\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in X],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in y],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXFcuSql4vSb",
        "colab_type": "text"
      },
      "source": [
        "The Bert model supports something called attention_mask, which is similar to the masking in keras. So here we create the mask to ignore the padded elements in the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXDMYUoW1ZjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI-p6IvZ3qVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EUQScmO5lnw",
        "colab_type": "text"
      },
      "source": [
        "### Building model\n",
        "\n",
        "#### Wrap the BERT model into a tensorflow keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoRuAJ6-3sOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            BERT_MODEL_HUB,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not (\"/cls/\" in var.name or 'pooler' in var.name)]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "        \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        \n",
        "        # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "        # Use \"sequence_outputs\" for token-level output.\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        return result['sequence_output']\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ufHi1bw7SGq",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSCIJvo7COQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "704554ea-1f8b-4d6a-936a-3dab798a3b16"
      },
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D, Lambda\n",
        "from tensorflow.keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D, add\n",
        "\n",
        "in_id = Input(shape=(MAX_LEN,), name=\"input_ids\")\n",
        "in_mask = Input(shape=(MAX_LEN,), name=\"input_masks\")\n",
        "in_segment = Input(shape=(MAX_LEN,), name=\"segment_ids\")\n",
        "bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "\n",
        "# Instantiate the custom Bert Layer defined above\n",
        "bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n",
        "\n",
        "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(bert_output)\n",
        "\n",
        "x_1 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "\n",
        "x_add = add([x, x_1])\n",
        "\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x_add)\n",
        "\n",
        "\n",
        "model = Model(bert_inputs, out)\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(clipnorm = 1.)\n",
        "model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 75)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 75)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 75)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_6 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, None, 1024)   5246976     bert_layer_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, None, 1024)   6295552     bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, 1024)   0           bidirectional_6[0][0]            \n",
            "                                                                 bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_6 (TimeDistrib (None, None, 17)     17425       add[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 120,491,349\n",
            "Trainable params: 17,465,105\n",
            "Non-trainable params: 103,026,244\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKXT5CE9dJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "87e91db4-d004-42d6-f6c3-2190006af6c5"
      },
      "source": [
        "tr_y = np.expand_dims(tr_tags, 2)\n",
        "val_y = np.expand_dims(val_tags, 2)\n",
        "batch_size = 32\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    history = model.fit([tr_inputs, np.array(tr_masks), np.zeros_like(tr_inputs)], tr_y, validation_data=([val_inputs, np.array(val_masks), np.zeros_like(val_inputs)], val_y),\n",
        "                                batch_size=batch_size, epochs=5, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 43163 samples, validate on 4796 samples\n",
            "Epoch 1/5\n",
            "43163/43163 [==============================] - 2111s 49ms/sample - loss: 0.0602 - acc: 0.9817 - val_loss: 0.0471 - val_acc: 0.9845\n",
            "Epoch 2/5\n",
            "14496/43163 [=========>....................] - ETA: 22:21 - loss: 0.0403 - acc: 0.9867"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQtCIIyk9MVG",
        "colab_type": "text"
      },
      "source": [
        "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUw3buna9Q3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}