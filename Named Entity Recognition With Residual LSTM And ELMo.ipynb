{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a residual LSTM network together with ELMo embeddings [1], developed at Allen NLP. You will learn how to wrap a tensorflow hub pre-trained model to work with keras. The resulting model with give you state-of-the-art performance on the named entity recognition task.\n",
    "\n",
    "### What are ELMo embeddings?\n",
    "ELMo embeddings are embeddings from a language model trained on the 1 Billion Word Benchmark and the pretrained version is availiable on tensorflow hub. Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence. They are computed on top of two-layer bidirectional language model with character convolutions, as a linear function of the internal network states. Concretely, ELMos use a pre-trained, multi-layer, bi-directional, LSTM-based language model and extract the hidden state of each layer for the input sequence of words. Then, they compute a weighted sum of those hidden states to obtain an embedding for each word. The weight of each hidden state is task-dependent and is learned. ELMo improves the performance of models across a wide range of tasks, spanning from question answering and sentiment analysis to named entity recognition. This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.\n",
    "\n",
    "I suggest having a look at the great paper “Deep contextualized word representations” https://arxiv.org/pdf/1802.05365.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences\n",
    "\n",
    "max_len = 50\n",
    "max_len_char = 10\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags); n_tags\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the EMLo embedding from tensorflow hub, we have to use strings as input. So we take the tokenized sentences and pad them to the desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iranian',\n",
       " 'officials',\n",
       " 'say',\n",
       " 'they',\n",
       " 'expect',\n",
       " 'to',\n",
       " 'get',\n",
       " 'access',\n",
       " 'to',\n",
       " 'sealed',\n",
       " 'sensitive',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'plant',\n",
       " 'Wednesday',\n",
       " ',',\n",
       " 'after',\n",
       " 'an',\n",
       " 'IAEA',\n",
       " 'surveillance',\n",
       " 'system',\n",
       " 'begins',\n",
       " 'functioning',\n",
       " '.',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__',\n",
       " '__PAD__']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[w[0] for w in s] for s in sentences]\n",
    "\n",
    "new_X = []\n",
    "for seq in X:\n",
    "    new_seq = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"__PAD__\")\n",
    "    new_X.append(new_seq)\n",
    "X = new_X\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  3,  4,\n",
       "        4,  4, 14,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ELMo residual LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'tensorflow_hub==0.4.0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ElmoEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable = False\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable, name=\"{}_module\".format(self.name))\n",
    "        self._trainable_weights += tf.compat.v1.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "            \n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]\n",
    "        return result\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '__PAD__')\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/Data/ethan/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/Data/ethan/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/Data/ethan/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/Data/ethan/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "elmo_embedding_layer (ElmoEmbed (32, None, 1024)     4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (32, None, 256)      1180672     elmo_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (32, None, 256)      394240      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (32, None, 256)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 17)     4369        add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 1,579,285\n",
      "Trainable params: 1,579,285\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D, Lambda\n",
    "from tensorflow.keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D, add\n",
    "\n",
    "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "#embedding = Lambda(ElmoEmbedding, output_shape=(None, max_len, 1024))(input_text)\n",
    "embedding = ElmoEmbeddingLayer()(input_text)\n",
    "x = Bidirectional(LSTM(units=128, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=128, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "\n",
    "model = Model(input_text, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val = X_tr[:1213*batch_size], X_tr[-135*batch_size:]\n",
    "y_tr, y_val = y_tr[:1213*batch_size], y_tr[-135*batch_size:]\n",
    "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
    "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38816 samples, validate on 4320 samples\n",
      "Epoch 1/5\n",
      "12320/38816 [========>.....................] - ETA: 5:04:09 - loss: 0.0863 - accuracy: 0.9460"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f2f3deec87f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),\n\u001b[0;32m----> 5\u001b[0;31m                         batch_size=batch_size, epochs=5, verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3465\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3466\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(tf.compat.v1.tables_initializer())\n",
    "    history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),\n",
    "                        batch_size=batch_size, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
